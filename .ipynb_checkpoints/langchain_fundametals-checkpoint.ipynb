{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f6b73fd-f675-4da2-922c-b6b186a11717",
   "metadata": {},
   "source": [
    "**IMPORTING DEPENDENCIES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d363c02-4e11-4b9f-ab45-5c966ade5294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- System & Environment ---\n",
    "import os                        # Operating system interface for file paths\n",
    "from dotenv import load_dotenv   # Loads environment variables from a .env file (API Keys)\n",
    "\n",
    "# --- Math & Utilities ---\n",
    "import numexpr                   # High-performance numerical expression evaluator for math tools\n",
    "\n",
    "# --- Core LLM & Models ---\n",
    "from langchain_groq import ChatGroq  # Groq's adapter for Llama 3 models\n",
    "\n",
    "# --- Prompts & Formatting ---\n",
    "from langchain_core.prompts import (\n",
    "    PromptTemplate,              # Template for standard text prompts\n",
    "    ChatPromptTemplate,          # Template for chat-based models (System, Human, AI)\n",
    "    MessagesPlaceholder          # Dynamic slot for injecting chat history into prompts\n",
    ")\n",
    "from langchain_core.output_parsers import StrOutputParser  # Converts model objects into clean strings\n",
    "\n",
    "# --- LCEL & Runnable Logic ---\n",
    "from langchain_core.runnables import RunnablePassthrough   # Passes inputs through the chain unchanged\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory  # Orchestrates memory automation\n",
    "\n",
    "# --- Agents & Tools ---\n",
    "from langchain.agents import create_agent               # Modern v1.x Graph-based agent constructor\n",
    "from langchain.tools import tool                        # Decorator to transform Python functions into LLM tools\n",
    "from langchain_community.tools import DuckDuckGoSearchRun # Free web search utility\n",
    "\n",
    "# --- Memory & History Management ---\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory # In-memory list to store chat logs\n",
    "from langchain_core.messages import trim_messages       # Utility to manage context window (e.g., k=1 logic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c3e0b55-8cee-4c9d-ba9b-159b2327a457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What an exciting venture. Here are 5 fancy name suggestions that blend Nigerian and Chinese cultures for your restaurant:\n",
      "\n",
      "1. **Jade Suya**: \"Jade\" is a symbol of good luck and prosperity in Chinese culture, while \"Suya\" is a popular Nigerian dish of grilled meat. This name combines the two, creating a unique and catchy title.\n",
      "2. **Peking Jollof**: \"Peking\" is a reference to Beijing, the capital city of China, while \"Jollof\" is a beloved Nigerian rice dish. This name brings together the flavors of both cultures in a single, elegant phrase.\n",
      "3. **Shanghai Akara**: \"Shanghai\" is one of China's most iconic cities, while \"Akara\" is a traditional Nigerian breakfast dish made from bean cakes. This name blends the modernity of Shanghai with the warmth of Nigerian cuisine.\n",
      "4. **Beijing Egusi**: \"Beijing\" represents the rich cultural heritage of China, while \"Egusi\" is a popular Nigerian soup made from melon seeds. This name combines the two, creating a sophisticated and exotic title.\n",
      "5. **Mandarin Mama Put**: \"Mandarin\" is a nod to the Chinese language and culture, while \"Mama Put\" is a colloquial Nigerian term for a street food vendor. This name playfully blends the two cultures, creating a fun and inviting atmosphere for your restaurant.\n",
      "\n",
      "These names are designed to evoke the rich cultural heritage of both Nigeria and China, while also sounding modern and appealing to a diverse customer base. Good luck with your restaurant venture!\n"
     ]
    }
   ],
   "source": [
    "# load Groq API Key\n",
    "load_dotenv()\n",
    "\n",
    "# initialize the Groq model\n",
    "# Llama 3.3 70B is one of the most powerful models they offer\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    temperature=0.6\n",
    ")\n",
    "\n",
    "# get your fancy restaurant names\n",
    "prompt = \"I want to open a Nigerian restaurant in China. Suggest 5 fancy names that blend both cultures.\"\n",
    "response = llm.invoke(prompt)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76cd8f12-8ef3-4ea8-84f0-0dcf104885e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to open a Nigerian restaurant in italy. Suggest 5 fancy names that blend both cultures.\n"
     ]
    }
   ],
   "source": [
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables = [\"country\"],\n",
    "    template = \"I want to open a Nigerian restaurant in {country}. Suggest 5 fancy names that blend both cultures.\"\n",
    ")\n",
    "\n",
    "# testing the format\n",
    "print(prompt_template_name.format(country='italy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d92deac5-0b4b-42a4-91b2-39ef9dc17148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What a fascinating fusion of cultures. Here are 5 fancy name suggestions for your Nigerian restaurant in Mexico:\n",
      "\n",
      "1. **Azul Lagos**: \"Azul\" means blue in Spanish, evoking the vibrant colors of Mexico, while \"Lagos\" is a nod to Nigeria's largest city, Lagos. The name combines the two cultures and sounds elegant.\n",
      "2. **Suya Fiesta**: \"Suya\" is a popular Nigerian dish, and \"Fiesta\" is a Mexican word that conveys celebration and joy. This name blends the flavors of Nigeria with the festive spirit of Mexico.\n",
      "3. **Naija Taqueria**: \"Naija\" is a colloquial term for Nigeria, while \"Taqueria\" is a Mexican word for a taco shop. This name combines the two cultures in a fun, casual way.\n",
      "4. **Abuja Cantina**: \"Abuja\" is the capital city of Nigeria, and \"Cantina\" is a Mexican word for a bar or restaurant. This name brings together the sophistication of Abuja with the warmth of a Mexican cantina.\n",
      "5. **Jollof Jalisco**: \"Jollof\" is a beloved Nigerian dish, and \"Jalisco\" is a state in Mexico known for its rich cultural heritage. This name combines the flavorful cuisine of Nigeria with the vibrant spirit of Jalisco, Mexico.\n",
      "\n",
      "These names are designed to evoke the rich cultural heritage of both Nigeria and Mexico, while also sounding elegant and sophisticated. Buen provecho! (Enjoy your meal!)\n"
     ]
    }
   ],
   "source": [
    "# create the \"Chain\" using the | operator (Modern LCEL way)\n",
    "chain = prompt_template_name | llm\n",
    "\n",
    "# run it\n",
    "response = chain.invoke({\"country\": \"Mexico\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fa9f190-ed8a-40f8-b0d3-11bd0f1d30ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azul Lagos\n",
      "Naija Fiesta\n",
      "Sahara Taco Bar\n",
      "Lagos Cantina\n",
      "Abuja Taqueria\n"
     ]
    }
   ],
   "source": [
    "# update the prompt to be strict\n",
    "template = \"\"\"\n",
    "You are a naming expert. \n",
    "I want to open a Nigerian restaurant in {country}. \n",
    "Suggest 5 fancy names that blend both cultures.\n",
    "\n",
    "CRITICAL INSTRUCTION: Return ONLY the list of 5 names. \n",
    "Do NOT include any introduction, \"Here are the names\", or conclusion. \n",
    "Just the names, one per line.\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# chain them together\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "# invoke\n",
    "response = chain.invoke({\"country\": \"Mexico\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e041d95c-8109-49ae-86b3-751ebc211142",
   "metadata": {},
   "source": [
    "### Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4baf3845-f2cb-48d4-a935-c4fb5034f712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- GENERATED NAMES ---\n",
      "Lagos Bistro Paris\n",
      "Naija Belle Époque\n",
      "Afrocrat Café Français\n",
      "\n",
      "--- SAMPLE MENU FOR ONE OF THE NAMES ---\n",
      "1. Jollof Coq au Vin - Chicken cooked in a rich jollof rice-infused red wine sauce.\n",
      "2. Suya Steak Tartare - Tender steak mixed with suya spices and served with crispy plantain chips.\n",
      "3. Egusi Bouillabaisse - A French fish stew with a Nigerian twist, featuring egusi seeds and assorted seafood.\n",
      "4. Puff-Puff Beignets - Fried dough pastries filled with a spicy Nigerian pepper sauce and powdered sugar.\n",
      "5. Akara Crème Brûlée - Fried bean cakes topped with a rich cream custard base and caramelized sugar.\n"
     ]
    }
   ],
   "source": [
    "# name generation\n",
    "name_prompt = PromptTemplate.from_template(\n",
    "    \"I want to open a Nigerian restaurant in {country}. \"\n",
    "    \"Suggest 3 fancy and unique names that blend both cultures. \"\n",
    "    \"Return ONLY the names, one per line, no extra text.\"\n",
    ")\n",
    "\n",
    "#step 1\n",
    "# menu generation\n",
    "# Note: this prompt takes 'restaurant_name' as input \n",
    "menu_prompt = PromptTemplate.from_template(\n",
    "    \"For a restaurant named '{restaurant_name}', suggest 5 gourmet menu items \"\n",
    "    \"that fuse Nigerian and local flavors. \"\n",
    "    \"Return ONLY the list of items with brief descriptions.\"\n",
    ")\n",
    "# step 2\n",
    "# create the Sequential Chain\n",
    "# use 'RunnablePassthrough' to carry the output of step 1 into step 2\n",
    "chain = (\n",
    "    {\"restaurant_name\": name_prompt | llm | parser} \n",
    "    | RunnablePassthrough.assign(menu=menu_prompt | llm | parser)\n",
    ")\n",
    "\n",
    "# run the chain\n",
    "result = chain.invoke({\"country\": \"France\"})\n",
    "\n",
    "# print the formatted output\n",
    "print(\"--- GENERATED NAMES ---\")\n",
    "print(result[\"restaurant_name\"])\n",
    "print(\"\\n--- SAMPLE MENU FOR ONE OF THE NAMES ---\")\n",
    "print(result[\"menu\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13912f3e-804c-4610-a301-175d76547388",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f134d957-6354-4f20-a304-fef93a3a9df6",
   "metadata": {},
   "source": [
    "### Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b2a2654-5cf6-45e5-8637-68d6447e5510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Answer ---\n",
      "The GDP of the US in 2025 is $27.61 trillion. Adding 5 to it gives $27,610,000,000,005.\n"
     ]
    }
   ],
   "source": [
    "# create a custom math tool using numexpr to replace the legacy llm-math library\n",
    "@tool\n",
    "def calculator(expression: str) -> str:\n",
    "    \"\"\"Useful for when you need to answer questions about math or perform \n",
    "    calculations. Input should be a mathematical expression.\"\"\"\n",
    "    try:\n",
    "        # evaluate the expression safely using numexpr\n",
    "        result = numexpr.evaluate(expression).item()\n",
    "        return str(result)\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}. Please ensure you provide a valid numerical expression.\"\n",
    "\n",
    "# setup search tool\n",
    "search = DuckDuckGoSearchRun()\n",
    "\n",
    "# combine the web search and custom calculator into a list of tools\n",
    "tools = [search, calculator]\n",
    "\n",
    "# initialize the modern Graph-based reasoning engine for the agent\n",
    "agent = create_agent(llm, tools)\n",
    "\n",
    "# run the agent\n",
    "try:\n",
    "    result = agent.invoke({\n",
    "        \"messages\": [(\"user\", \"What was the GDP of the US in 2025? Add 5 to it.\")]\n",
    "    })\n",
    "\n",
    "    print(\"\\n--- Final Answer ---\")\n",
    "    print(result[\"messages\"][-1].content)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce8e90b-a164-4cd4-82e1-316140fec36f",
   "metadata": {},
   "source": [
    "### Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fd4528-05fb-4258-9a0d-48e1e1e4a3d4",
   "metadata": {},
   "source": [
    "#### Basic Memory (ConversationBufferMemory equivalent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0eb6670-fae6-4a2e-a3d6-d593ae6e3a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Mexican Response ---\n",
      "1. Fiesta Royale\n",
      "2. Azul Casa\n",
      "3. El Jardin de Oro\n",
      "4. Casa de Sol y Fuego\n",
      "5. Viva la Vida Cocina\n",
      "\n",
      "--- Arabic Response ---\n",
      "1. Al Jazeera Palace\n",
      "2. Sahara Nights\n",
      "3. Mashreq Manor\n",
      "4. Sultan's Table\n",
      "5. Aladdin's Oasis\n"
     ]
    }
   ],
   "source": [
    "# Updated the LLM temperature for slightly more creative responses\n",
    "llm = ChatGroq(model=\"llama-3.3-70b-versatile\", temperature=0.7)\n",
    "\n",
    "# set up the consultant persona and strict rules to keep the output clean\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are a professional naming consultant. \n",
    "    Suggest 5 fancy names for the requested cuisine.\n",
    "    \n",
    "    RULES:\n",
    "    - Return ONLY the list of names.\n",
    "    - No introductions, no descriptions, and no concluding remarks.\n",
    "    - Format: 1. Name, 2. Name...\"\"\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"Cuisine: {cuisine}\"),\n",
    "])\n",
    "\n",
    "# create the pipeline and add a parser to get back clean text instead of a raw object\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# set up an in-memory storage to keep track of our conversation history\n",
    "history = ChatMessageHistory()\n",
    "\n",
    "# attach memory logic to the chain so it automatically tracks the back-and-forth\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    lambda session_id: history,\n",
    "    input_messages_key=\"cuisine\",\n",
    "    history_messages_key=\"history\",\n",
    ")\n",
    "\n",
    "# --- Start Generating Names ---\n",
    "\n",
    "# request suggestions for Mexican cuisine under session 123\n",
    "response1 = chain_with_history.invoke(\n",
    "    {\"cuisine\": \"Mexican\"}, \n",
    "    config={\"configurable\": {\"session_id\": \"123\"}}\n",
    ")\n",
    "print(f\"--- Mexican Response ---\\n{response1}\")\n",
    "\n",
    "# request Arabic names and ensure the model remembers the previous turn\n",
    "response2 = chain_with_history.invoke(\n",
    "    {\"cuisine\": \"Arabic\"}, \n",
    "    config={\"configurable\": {\"session_id\": \"123\"}}\n",
    ")\n",
    "print(f\"\\n--- Arabic Response ---\\n{response2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753ca6d0-8588-4349-a8a6-a2c7d4dd566d",
   "metadata": {},
   "source": [
    "#### ConversationChain (The Chatbot logic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fab94450-f716-403d-9203-b5fc08093577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STARTING CONVERSATION ===\n",
      "\n",
      "[USER]: Who won the first cricket world cup?\n",
      "[AI]:   The West Indies won the first cricket World Cup in 1975.\n",
      "--------------------------------------------------\n",
      "\n",
      "[USER]: How much is 5+5?\n",
      "[AI]:   5 + 5 = 10.\n",
      "--------------------------------------------------\n",
      "\n",
      "[USER]: Who was the captain of the winning team?\n",
      "[AI]:   Clive Lloyd was the captain of the West Indies team that won the 1975 Cricket World Cup.\n",
      "--------------------------------------------------\n",
      "\n",
      "=== FINAL MEMORY BUFFER LOG ===\n",
      "[Human]: Who won the first cricket world cup?...\n",
      "[AI]: The West Indies won the first cricket World Cup in 1975....\n",
      "[Human]: How much is 5+5?...\n",
      "[AI]: 5 + 5 = 10....\n",
      "[Human]: Who was the captain of the winning team?...\n",
      "[AI]: Clive Lloyd was the captain of the West Indies team that won...\n"
     ]
    }
   ],
   "source": [
    "# set up the conversation flow with a focus on keeping things brief and friendly\n",
    "convo_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a concise, friendly AI. Answer accurately and briefly.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "\n",
    "# initialize our storage and link the prompt to the model\n",
    "convo_history = ChatMessageHistory()\n",
    "convo_chain = convo_prompt | llm\n",
    "\n",
    "# create a helper function to manage the chat loop and keep the console output clean\n",
    "def chat(user_input):\n",
    "    # pull the existing conversation history so the AI has context\n",
    "    history_messages = convo_history.messages\n",
    "    \n",
    "    # send the history and the new question to the AI\n",
    "    response = convo_chain.invoke({\"input\": user_input, \"history\": history_messages})\n",
    "    \n",
    "    # log both the question and the answer into our memory storage\n",
    "    convo_history.add_user_message(user_input)\n",
    "    convo_history.add_ai_message(response.content)\n",
    "    \n",
    "    # display the interaction with clear labels and a divider for readability\n",
    "    print(f\"\\n[USER]: {user_input}\")\n",
    "    print(f\"[AI]:   {response.content}\")\n",
    "    print(\"-\" * 50) \n",
    "\n",
    "# start the actual dialogue\n",
    "print(\"=== STARTING CONVERSATION ===\")\n",
    "chat(\"Who won the first cricket world cup?\")\n",
    "chat(\"How much is 5+5?\")\n",
    "chat(\"Who was the captain of the winning team?\")\n",
    "\n",
    "# print out a quick snapshot of the memory buffer to verify it's working\n",
    "print(\"\\n=== FINAL MEMORY BUFFER LOG ===\")\n",
    "for msg in convo_history.messages:\n",
    "    role = \"Human\" if msg.type == \"human\" else \"AI\"\n",
    "    # show just the start of each message to keep the log tidy\n",
    "    print(f\"[{role}]: {msg.content[:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155e4c9e-d0d3-4864-b298-34a4254cc738",
   "metadata": {},
   "source": [
    "#### ConversationBufferWindowMemory (k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ac4301d-021e-492d-b7ce-e31a3e38daad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Start Window Memory (k=1) ---\n",
      "Q1: The West Indies won the first cricket World Cup in 1975.\n",
      "Q2: 5 + 5 = 10.\n",
      "Q3: There's no team mentioned. Could you provide more context?\n"
     ]
    }
   ],
   "source": [
    "# create a sliding window for memory to keep only the most recent interactions\n",
    "def chat_with_window(user_input, k=1):\n",
    "    # pull the full record of our conversation so far\n",
    "    current_history = convo_history.messages\n",
    "    \n",
    "    # slice the history to keep only the last 'k' rounds (1 round = 1 question + 1 answer)\n",
    "    trimmed_history = current_history[-(k*2):] if k > 0 else []\n",
    "    \n",
    "    # run the model using only this limited \"short-term\" memory\n",
    "    response = convo_chain.invoke({\"input\": user_input, \"history\": trimmed_history})\n",
    "    \n",
    "    # save the new exchange to the main history so we can track the \"forgetting\" effect\n",
    "    convo_history.add_user_message(user_input)\n",
    "    convo_history.add_ai_message(response.content)\n",
    "    \n",
    "    return response.content\n",
    "\n",
    "# wipe the slate clean for a fresh test of the window logic\n",
    "convo_history.clear()\n",
    "\n",
    "\n",
    "\n",
    "print(\"--- Start Window Memory (k=1) ---\")\n",
    "print(f\"Q1: {chat_with_window('Who won the first cricket world cup?')}\")\n",
    "print(f\"Q2: {chat_with_window('How much is 5+5?')}\") \n",
    "\n",
    "# since k=1, the AI now only sees the 5+5 exchange and has \"forgotten\" the cricket context\n",
    "print(f\"Q3: {chat_with_window('Who was the captain of the winning team?')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63db81b2-d462-40ef-a216-25b5cb1adb0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb9dd57-d232-406e-882f-ef68d195b63d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (dev_env)",
   "language": "python",
   "name": "dev_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
